{"cells":[{"cell_type":"code","execution_count":3,"metadata":{"id":"gi60YmV0K7xO","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1740722492511,"user_tz":360,"elapsed":172935,"user":{"displayName":"Andrew Castillo","userId":"02822267340440655112"}},"outputId":"18d806d0-0e23-4d83-a33f-72bb71e9e43f"},"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n","ðŸ”¹ Processing year: 2020\n","ðŸ”¹ Processing year: 2021\n","ðŸ”¹ Processing year: 2022\n","ðŸ”¹ Processing year: 2023\n","ðŸ”¹ Processing year: 2024\n","ðŸ”¹ Processing year: 2025\n","Error processing 202502-divvy-tripdata.zip: 404 Client Error: Not Found for url: https://divvy-tripdata.s3.amazonaws.com/202502-divvy-tripdata.zip\n","Error processing 202503-divvy-tripdata.zip: 404 Client Error: Not Found for url: https://divvy-tripdata.s3.amazonaws.com/202503-divvy-tripdata.zip\n","Error processing 202504-divvy-tripdata.zip: 404 Client Error: Not Found for url: https://divvy-tripdata.s3.amazonaws.com/202504-divvy-tripdata.zip\n","Error processing 202505-divvy-tripdata.zip: 404 Client Error: Not Found for url: https://divvy-tripdata.s3.amazonaws.com/202505-divvy-tripdata.zip\n","Error processing 202507-divvy-tripdata.zip: 404 Client Error: Not Found for url: https://divvy-tripdata.s3.amazonaws.com/202507-divvy-tripdata.zip\n","Error processing 202506-divvy-tripdata.zip: 404 Client Error: Not Found for url: https://divvy-tripdata.s3.amazonaws.com/202506-divvy-tripdata.zip\n","Error processing 202509-divvy-tripdata.zip: 404 Client Error: Not Found for url: https://divvy-tripdata.s3.amazonaws.com/202509-divvy-tripdata.zip\n","Error processing 202510-divvy-tripdata.zip: 404 Client Error: Not Found for url: https://divvy-tripdata.s3.amazonaws.com/202510-divvy-tripdata.zip\n","Error processing 202508-divvy-tripdata.zip: 404 Client Error: Not Found for url: https://divvy-tripdata.s3.amazonaws.com/202508-divvy-tripdata.zip\n","Error processing 202511-divvy-tripdata.zip: 404 Client Error: Not Found for url: https://divvy-tripdata.s3.amazonaws.com/202511-divvy-tripdata.zip\n","Error processing 202512-divvy-tripdata.zip: 404 Client Error: Not Found for url: https://divvy-tripdata.s3.amazonaws.com/202512-divvy-tripdata.zip\n"]}],"source":["import os\n","import requests\n","import zipfile\n","import pandas as pd\n","from io import BytesIO\n","from concurrent.futures import ThreadPoolExecutor\n","from google.colab import drive\n","\n","# âœ… Mount Google Drive\n","drive.mount('/content/drive')\n","\n","# âœ… Define Paths\n","SHARED_DRIVE_NAME = \"Time Series\"\n","DATA_DIR = f\"/content/drive/Shared drives/{SHARED_DRIVE_NAME}/divvy_data/prod\"\n","os.makedirs(DATA_DIR, exist_ok=True)\n","\n","BASE_URL = \"https://divvy-tripdata.s3.amazonaws.com/\"\n","YEARS = range(2020, 2026)\n","\n","parquet_path_all = os.path.join(DATA_DIR, \"divvy_data_expanded.parquet\")\n","csv_path_all = os.path.join(DATA_DIR, \"divvy_data_station.csv\")\n","\n","# âœ… Function to download and extract data\n","def download_and_extract(year, month=None):\n","    \"\"\"Downloads and extracts Divvy data for a given year and month (or quarter for 2020 Q1).\"\"\"\n","    filename = \"Divvy_Trips_2020_Q1.zip\" if year == 2020 and month is None else f\"{year}{month:02d}-divvy-tripdata.zip\"\n","    try:\n","        response = requests.get(f\"{BASE_URL}{filename}\", stream=True, timeout=10)\n","        response.raise_for_status()\n","        with zipfile.ZipFile(BytesIO(response.content)) as z:\n","            with z.open(z.namelist()[0]) as csvfile:\n","                df = pd.read_csv(csvfile, usecols=[\n","                    \"ride_id\", \"started_at\", \"rideable_type\", \"start_station_name\",\n","                    \"end_station_name\", \"start_station_id\", \"end_station_id\", \"start_lat\",\n","                    \"start_lng\", \"end_lat\", \"end_lng\", \"member_casual\"\n","                ], dtype={\"start_station_id\": str, \"end_station_id\": str})\n","\n","                # âœ… Handle missing station IDs\n","                # df[\"start_station_id\"].fillna(\"\", inplace=True)\n","                # df[\"end_station_id\"].fillna(\"\", inplace=True)\n","\n","                # âœ… Assign 'Outside of Dock' for electric bikes without a dock\n","                df.loc[(df[\"rideable_type\"] == \"electric_bike\") & df[\"start_station_name\"].isna(), \"start_station_name\"] = \"Outside of Dock\"\n","                df.loc[(df[\"rideable_type\"] == \"electric_bike\") & df[\"end_station_name\"].isna(), \"end_station_name\"] = \"Outside of Dock\"\n","\n","                # âœ… Convert to date format\n","                df[\"date\"] = pd.to_datetime(df[\"started_at\"], errors=\"coerce\").dt.date\n","                return df\n","    except Exception as e:\n","        print(f\"Error processing {filename}: {e}\")\n","        return None\n","\n","# âœ… Process Data Year by Year\n","all_years_data = []\n","\n","for year in YEARS:\n","    print(f\"ðŸ”¹ Processing year: {year}\")\n","    all_data = []\n","    with ThreadPoolExecutor(max_workers=10) as executor:\n","        futures = []\n","        if year == 2020:\n","            futures.append(executor.submit(download_and_extract, 2020, None))\n","        for month in range(1, 13):\n","            if year == 2020 and month <= 3:\n","                continue\n","            futures.append(executor.submit(download_and_extract, year, month))\n","        for future in futures:\n","            result = future.result()\n","            if result is not None:\n","                all_data.append(result)\n","\n","    if all_data:\n","        df_year = pd.concat(all_data, ignore_index=True)\n","        df_year[\"date\"] = pd.to_datetime(df_year[\"date\"])\n","        df_year[\"e_bike_id\"] = df_year[\"ride_id\"].where(df_year[\"rideable_type\"] == \"electric_bike\")\n","\n","        # âœ… Aggregate distinct ride counts\n","        grouped_df = df_year.groupby([\n","            \"date\", \"start_station_name\",\n","            \"start_station_id\"\n","        ]).agg(\n","            total_rides=(\"ride_id\", \"nunique\"),\n","            ebike_rides=(\"e_bike_id\", \"nunique\")\n","        ).reset_index()\n","\n","        # âœ… Collect all data for final full dataset\n","        all_years_data.append(grouped_df)\n","\n","# âœ… Save Full Dataset (All Years)\n","if all_years_data:\n","    df_all = pd.concat(all_years_data, ignore_index=True)\n","    df_all[\"date\"] = pd.to_datetime(df_all[\"date\"])\n","    df_all = df_all.drop(columns=[\"ebike_rides\"])\n","    # Compute proportion of e-bike rides\n","    # df_all[\"ebike_shift\"] = df_all[\"ebike_rides\"].shift(1)\n","    # df_all[\"ebike_shift_proportion\"] = df_all[\"ebike_rides\"].shift(1) / df_all[\"total_rides\"].shift(1)\n","    # Compute moving average of e-bike rides\n","    # df_all[\"ebike_proportion_ma365\"] = df_all[\"ebike_shift_proportion\"].rolling(window=365, min_periods=1).mean()\n","    # df_all[\"ebike_proportion_ma30\"] = df_all[\"ebike_shift_proportion\"].rolling(window=30, min_periods=1).mean()\n","\n","    # âœ… Load weather data\n","    weather_df = pd.read_csv(\"/content/drive/Shared drives/Time Series/weather_data/daily_weather_forecast_chicago.csv\")\n","    if \"time\" in weather_df.columns:\n","        weather_df[\"date\"] = pd.to_datetime(weather_df[\"time\"])\n","        weather_df = weather_df[[\"date\", \"temp_min_c\", \"rain_sum_mm\", \"snowfall_sum_cm\"]]\n","        df_all = df_all.merge(weather_df, on=\"date\", how=\"left\")\n","\n","    # âœ… Train Test split\n","    def train_test_split_by_date(df, split_date=\"2024-01-01\"):\n","        train = df[df[\"date\"] < split_date]\n","        test = df[df[\"date\"] >= split_date]\n","        return train, test\n","\n","    def add_date_features(df):\n","        data_raw = df.copy()\n","        data_raw[\"date\"] = pd.to_datetime(data_raw[\"date\"])\n","        data_raw.sort_values(\"date\",inplace=True)\n","        data_raw[\"month\"] = data_raw[\"date\"].dt.month\n","        data_raw[\"dayofweek\"] = data_raw[\"date\"].dt.dayofweek\n","        data_raw[\"year\"] = data_raw[\"date\"].dt.year\n","        return data_raw\n","\n","    df_all = add_date_features(df_all)\n","\n","    train, test = train_test_split_by_date(df_all)\n","\n","    # âœ… Save datasets\n","    train.to_csv(os.path.join(DATA_DIR, \"station\", \"divvy_station_train.csv\"), index=False)\n","    test.to_csv(os.path.join(DATA_DIR, \"station\", \"divvy_station_test.csv\"), index=False)\n"]},{"cell_type":"code","source":[],"metadata":{"id":"krfCutu66q7T"},"execution_count":null,"outputs":[]}],"metadata":{"colab":{"provenance":[],"machine_shape":"hm"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}